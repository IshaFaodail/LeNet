{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28f01082",
   "metadata": {},
   "outputs": [],
   "source": [
    "##combine data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e84d05de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Hp\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import linalg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46bb5a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the mnist dataset\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "# Add a channels dimension\n",
    "x_train = x_train[..., tf.newaxis].astype(\"float32\")\n",
    "x_test = x_test[..., tf.newaxis].astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffa5f854",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28, 1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73859bee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28, 1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0].shape    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b7f09f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x226e55ddb90>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGdCAYAAABU0qcqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAcTUlEQVR4nO3df3DU9b3v8dcCyQqaLI0hv0rAgD+wAvEWJWZAxJJLSOc4gIwHf3QGvF4cMXiKaPXGUZHWM2nxjrV6qd7TqURnxB+cEaiO5Y4GE441oQNKGW7blNBY4iEJFSe7IUgIyef+wXXrQgJ+1l3eSXg+Zr4zZPf75vvx69Znv9nNNwHnnBMAAOfYMOsFAADOTwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYGGG9gFP19vbq4MGDSktLUyAQsF4OAMCTc04dHR3Ky8vTsGH9X+cMuAAdPHhQ+fn51ssAAHxDzc3NGjt2bL/PD7gApaWlSZJm6vsaoRTj1QAAfJ1Qtz7QO9H/nvcnaQFat26dnnrqKbW2tqqwsFDPPfecpk+ffta5L7/tNkIpGhEgQAAw6Pz/O4ye7W2UpHwI4fXXX9eqVau0evVqffTRRyosLFRpaakOHTqUjMMBAAahpATo6aef1rJly3TnnXfqO9/5jl544QWNGjVKL774YjIOBwAYhBIeoOPHj2vXrl0qKSn5x0GGDVNJSYnq6upO27+rq0uRSCRmAwAMfQkP0Geffaaenh5lZ2fHPJ6dna3W1tbT9q+srFQoFIpufAIOAM4P5j+IWlFRoXA4HN2am5utlwQAOAcS/im4zMxMDR8+XG1tbTGPt7W1KScn57T9g8GggsFgopcBABjgEn4FlJqaqmnTpqm6ujr6WG9vr6qrq1VcXJzowwEABqmk/BzQqlWrtGTJEl1zzTWaPn26nnnmGXV2durOO+9MxuEAAINQUgK0ePFi/f3vf9fjjz+u1tZWXX311dq6detpH0wAAJy/As45Z72Ir4pEIgqFQpqt+dwJAQAGoROuWzXaonA4rPT09H73M/8UHADg/ESAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYGGG9AGAgCYzw/5/E8DGZSVhJYjQ8eElccz2jer1nxk885D0z6t6A90zr06neMx9d87r3jCR91tPpPVO08QHvmUtX1XvPDAVcAQEATBAgAICJhAfoiSeeUCAQiNkmTZqU6MMAAAa5pLwHdNVVV+m99977x0Hi+L46AGBoS0oZRowYoZycnGT81QCAISIp7wHt27dPeXl5mjBhgu644w4dOHCg3327uroUiURiNgDA0JfwABUVFamqqkpbt27V888/r6amJl1//fXq6Ojoc//KykqFQqHolp+fn+glAQAGoIQHqKysTLfccoumTp2q0tJSvfPOO2pvb9cbb7zR5/4VFRUKh8PRrbm5OdFLAgAMQEn/dMDo0aN1+eWXq7Gxsc/ng8GggsFgspcBABhgkv5zQEeOHNH+/fuVm5ub7EMBAAaRhAfowQcfVG1trT755BN9+OGHWrhwoYYPH67bbrst0YcCAAxiCf8W3KeffqrbbrtNhw8f1pgxYzRz5kzV19drzJgxiT4UAGAQS3iAXnvttUT/lRighl95mfeMC6Z4zxy8YbT3zBfX+d9EUpIyQv5z/1EY340uh5rfHk3znvnZ/5rnPbNjygbvmabuL7xnJOmnbf/VeybvP1xcxzofcS84AIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMBE0n8hHQa+ntnfjWvu6ap13jOXp6TGdSycW92ux3vm8eeWes+M6PS/cWfxxhXeM2n/ecJ7RpKCn/nfxHTUzh1xHet8xBUQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATHA3bCjYcDCuuV3H8r1nLk9pi+tYQ80DLdd5z/z1SKb3TNXEf/eekaRwr/9dqrOf/TCuYw1k/mcBPrgCAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMcDNS6ERLa1xzz/3sFu+Zf53X6T0zfM9F3jN/uPc575l4PfnZVO+ZxpJR3jM97S3eM7cX3+s9I0mf/Iv/TIH+ENexcP7iCggAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMMHNSBG3jPV13jNj3rrYe6bn8OfeM1dN/m/eM5L0f2e96D3zm3+7wXsmq/1D75l4BOriu0Fogf+/WsAbV0AAABMECABgwjtA27dv10033aS8vDwFAgFt3rw55nnnnB5//HHl5uZq5MiRKikp0b59+xK1XgDAEOEdoM7OThUWFmrdunV9Pr927Vo9++yzeuGFF7Rjxw5deOGFKi0t1bFjx77xYgEAQ4f3hxDKyspUVlbW53POOT3zzDN69NFHNX/+fEnSyy+/rOzsbG3evFm33nrrN1stAGDISOh7QE1NTWptbVVJSUn0sVAopKKiItXV9f2xmq6uLkUikZgNADD0JTRAra2tkqTs7OyYx7Ozs6PPnaqyslKhUCi65efnJ3JJAIAByvxTcBUVFQqHw9GtubnZekkAgHMgoQHKycmRJLW1tcU83tbWFn3uVMFgUOnp6TEbAGDoS2iACgoKlJOTo+rq6uhjkUhEO3bsUHFxcSIPBQAY5Lw/BXfkyBE1NjZGv25qatLu3buVkZGhcePGaeXKlXryySd12WWXqaCgQI899pjy8vK0YMGCRK4bADDIeQdo586duvHGG6Nfr1q1SpK0ZMkSVVVV6aGHHlJnZ6fuvvtutbe3a+bMmdq6dasuuOCCxK0aADDoBZxzznoRXxWJRBQKhTRb8zUikGK9HAxSf/nf18Y3908veM/c+bc53jN/n9nhPaPeHv8ZwMAJ160abVE4HD7j+/rmn4IDAJyfCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYML71zEAg8GVD/8lrrk7p/jf2Xr9+Oqz73SKG24p955Je73eewYYyLgCAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMcDNSDEk97eG45g4vv9J75sBvvvCe+R9Pvuw9U/HPC71n3Mch7xlJyv/XOv8h5+I6Fs5fXAEBAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACa4GSnwFb1/+JP3zK1rfuQ988rq/+k9s/s6/xuY6jr/EUm66sIV3jOX/arFe+bEXz/xnsHQwRUQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGAi4Jxz1ov4qkgkolAopNmarxGBFOvlAEnhZlztPZP+00+9Z16d8H+8Z+I16f3/7j1zxZqw90zPvr96z+DcOuG6VaMtCofDSk9P73c/roAAACYIEADAhHeAtm/frptuukl5eXkKBALavHlzzPNLly5VIBCI2ebNm5eo9QIAhgjvAHV2dqqwsFDr1q3rd5958+appaUlur366qvfaJEAgKHH+zeilpWVqays7Iz7BINB5eTkxL0oAMDQl5T3gGpqapSVlaUrrrhCy5cv1+HDh/vdt6urS5FIJGYDAAx9CQ/QvHnz9PLLL6u6ulo/+9nPVFtbq7KyMvX09PS5f2VlpUKhUHTLz89P9JIAAAOQ97fgzubWW2+N/nnKlCmaOnWqJk6cqJqaGs2ZM+e0/SsqKrRq1aro15FIhAgBwHkg6R/DnjBhgjIzM9XY2Njn88FgUOnp6TEbAGDoS3qAPv30Ux0+fFi5ubnJPhQAYBDx/hbckSNHYq5mmpqatHv3bmVkZCgjI0Nr1qzRokWLlJOTo/379+uhhx7SpZdeqtLS0oQuHAAwuHkHaOfOnbrxxhujX3/5/s2SJUv0/PPPa8+ePXrppZfU3t6uvLw8zZ07Vz/5yU8UDAYTt2oAwKDHzUiBQWJ4dpb3zMHFl8Z1rB0P/8J7Zlgc39G/o2mu90x4Zv8/1oGBgZuRAgAGNAIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJhI+K/kBpAcPW2HvGeyn/WfkaRjD53wnhkVSPWe+dUlb3vP/NPCld4zozbt8J5B8nEFBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCY4GakgIHemVd7z+y/5QLvmclXf+I9I8V3Y9F4PPf5f/GeGbVlZxJWAgtcAQEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJrgZKfAVgWsme8/85V/8b9z5qxkvec/MuuC498y51OW6vWfqPy/wP1Bvi/8MBiSugAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAE9yMFAPeiILx3jP778yL61hPLH7Ne2bRRZ/FdayB7JG2a7xnan9xnffMt16q857B0MEVEADABAECAJjwClBlZaWuvfZapaWlKSsrSwsWLFBDQ0PMPseOHVN5ebkuvvhiXXTRRVq0aJHa2toSumgAwODnFaDa2lqVl5ervr5e7777rrq7uzV37lx1dnZG97n//vv11ltvaePGjaqtrdXBgwd18803J3zhAIDBzetDCFu3bo35uqqqSllZWdq1a5dmzZqlcDisX//619qwYYO+973vSZLWr1+vK6+8UvX19bruOv83KQEAQ9M3eg8oHA5LkjIyMiRJu3btUnd3t0pKSqL7TJo0SePGjVNdXd+fdunq6lIkEonZAABDX9wB6u3t1cqVKzVjxgxNnjxZktTa2qrU1FSNHj06Zt/s7Gy1trb2+fdUVlYqFApFt/z8/HiXBAAYROIOUHl5ufbu3avXXvP/uYmvqqioUDgcjm7Nzc3f6O8DAAwOcf0g6ooVK/T2229r+/btGjt2bPTxnJwcHT9+XO3t7TFXQW1tbcrJyenz7woGgwoGg/EsAwAwiHldATnntGLFCm3atEnbtm1TQUFBzPPTpk1TSkqKqquro481NDTowIEDKi4uTsyKAQBDgtcVUHl5uTZs2KAtW7YoLS0t+r5OKBTSyJEjFQqFdNddd2nVqlXKyMhQenq67rvvPhUXF/MJOABADK8APf/885Kk2bNnxzy+fv16LV26VJL085//XMOGDdOiRYvU1dWl0tJS/fKXv0zIYgEAQ0fAOeesF/FVkUhEoVBIszVfIwIp1svBGYy4ZJz3THharvfM4h9vPftOp7hn9F+9Zwa6B1r8v4tQ90v/m4pKUkbV7/2HenviOhaGnhOuWzXaonA4rPT09H73415wAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMBHXb0TFwDUit+/fPHsmn794YVzHWl5Q6z1zW1pbXMcayFb850zvmY+ev9p7JvPf93rPZHTUec8A5wpXQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACW5Geo4cL73Gf+b+z71nHrn0He+ZuSM7vWcGuraeL+Kam/WbB7xnJj36Z++ZjHb/m4T2ek8AAxtXQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACW5Geo58ssC/9X+ZsjEJK0mcde0TvWd+UTvXeybQE/CemfRkk/eMJF3WtsN7pieuIwHgCggAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMBFwzjnrRXxVJBJRKBTSbM3XiECK9XIAAJ5OuG7VaIvC4bDS09P73Y8rIACACQIEADDhFaDKykpde+21SktLU1ZWlhYsWKCGhoaYfWbPnq1AIBCz3XPPPQldNABg8PMKUG1trcrLy1VfX693331X3d3dmjt3rjo7O2P2W7ZsmVpaWqLb2rVrE7poAMDg5/UbUbdu3RrzdVVVlbKysrRr1y7NmjUr+vioUaOUk5OTmBUCAIakb/QeUDgcliRlZGTEPP7KK68oMzNTkydPVkVFhY4ePdrv39HV1aVIJBKzAQCGPq8roK/q7e3VypUrNWPGDE2ePDn6+O23367x48crLy9Pe/bs0cMPP6yGhga9+eabff49lZWVWrNmTbzLAAAMUnH/HNDy5cv129/+Vh988IHGjh3b737btm3TnDlz1NjYqIkTJ572fFdXl7q6uqJfRyIR5efn83NAADBIfd2fA4rrCmjFihV6++23tX379jPGR5KKiookqd8ABYNBBYPBeJYBABjEvALknNN9992nTZs2qaamRgUFBWed2b17tyQpNzc3rgUCAIYmrwCVl5drw4YN2rJli9LS0tTa2ipJCoVCGjlypPbv368NGzbo+9//vi6++GLt2bNH999/v2bNmqWpU6cm5R8AADA4eb0HFAgE+nx8/fr1Wrp0qZqbm/WDH/xAe/fuVWdnp/Lz87Vw4UI9+uijZ/w+4FdxLzgAGNyS8h7Q2VqVn5+v2tpan78SAHCe4l5wAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATI6wXcCrnnCTphLolZ7wYAIC3E+qW9I//nvdnwAWoo6NDkvSB3jFeCQDgm+jo6FAoFOr3+YA7W6LOsd7eXh08eFBpaWkKBAIxz0UiEeXn56u5uVnp6elGK7THeTiJ83AS5+EkzsNJA+E8OOfU0dGhvLw8DRvW/zs9A+4KaNiwYRo7duwZ90lPTz+vX2Bf4jycxHk4ifNwEufhJOvzcKYrny/xIQQAgAkCBAAwMagCFAwGtXr1agWDQeulmOI8nMR5OInzcBLn4aTBdB4G3IcQAADnh0F1BQQAGDoIEADABAECAJggQAAAE4MmQOvWrdMll1yiCy64QEVFRfr9739vvaRz7oknnlAgEIjZJk2aZL2spNu+fbtuuukm5eXlKRAIaPPmzTHPO+f0+OOPKzc3VyNHjlRJSYn27dtns9gkOtt5WLp06Wmvj3nz5tksNkkqKyt17bXXKi0tTVlZWVqwYIEaGhpi9jl27JjKy8t18cUX66KLLtKiRYvU1tZmtOLk+DrnYfbs2ae9Hu655x6jFfdtUATo9ddf16pVq7R69Wp99NFHKiwsVGlpqQ4dOmS9tHPuqquuUktLS3T74IMPrJeUdJ2dnSosLNS6dev6fH7t2rV69tln9cILL2jHjh268MILVVpaqmPHjp3jlSbX2c6DJM2bNy/m9fHqq6+ewxUmX21trcrLy1VfX693331X3d3dmjt3rjo7O6P73H///Xrrrbe0ceNG1dbW6uDBg7r55psNV514X+c8SNKyZctiXg9r1641WnE/3CAwffp0V15eHv26p6fH5eXlucrKSsNVnXurV692hYWF1sswJclt2rQp+nVvb6/LyclxTz31VPSx9vZ2FwwG3auvvmqwwnPj1PPgnHNLlixx8+fPN1mPlUOHDjlJrra21jl38t99SkqK27hxY3SfP/3pT06Sq6urs1pm0p16Hpxz7oYbbnA//OEP7Rb1NQz4K6Djx49r165dKikpiT42bNgwlZSUqK6uznBlNvbt26e8vDxNmDBBd9xxhw4cOGC9JFNNTU1qbW2NeX2EQiEVFRWdl6+PmpoaZWVl6YorrtDy5ct1+PBh6yUlVTgcliRlZGRIknbt2qXu7u6Y18OkSZM0bty4If16OPU8fOmVV15RZmamJk+erIqKCh09etRief0acDcjPdVnn32mnp4eZWdnxzyenZ2tP//5z0arslFUVKSqqipdccUVamlp0Zo1a3T99ddr7969SktLs16eidbWVknq8/Xx5XPni3nz5unmm29WQUGB9u/fr0ceeURlZWWqq6vT8OHDrZeXcL29vVq5cqVmzJihyZMnSzr5ekhNTdXo0aNj9h3Kr4e+zoMk3X777Ro/frzy8vK0Z88ePfzww2poaNCbb75puNpYAz5A+IeysrLon6dOnaqioiKNHz9eb7zxhu666y7DlWEguPXWW6N/njJliqZOnaqJEyeqpqZGc+bMMVxZcpSXl2vv3r3nxfugZ9Lfebj77rujf54yZYpyc3M1Z84c7d+/XxMnTjzXy+zTgP8WXGZmpoYPH37ap1ja2tqUk5NjtKqBYfTo0br88svV2NhovRQzX74GeH2cbsKECcrMzBySr48VK1bo7bff1vvvvx/z61tycnJ0/Phxtbe3x+w/VF8P/Z2HvhQVFUnSgHo9DPgApaamatq0aaquro4+1tvbq+rqahUXFxuuzN6RI0e0f/9+5ebmWi/FTEFBgXJycmJeH5FIRDt27DjvXx+ffvqpDh8+PKReH845rVixQps2bdK2bdtUUFAQ8/y0adOUkpIS83poaGjQgQMHhtTr4WznoS+7d++WpIH1erD+FMTX8dprr7lgMOiqqqrcH//4R3f33Xe70aNHu9bWVuulnVMPPPCAq6mpcU1NTe53v/udKykpcZmZme7QoUPWS0uqjo4O9/HHH7uPP/7YSXJPP/20+/jjj93f/vY355xzP/3pT93o0aPdli1b3J49e9z8+fNdQUGB++KLL4xXnlhnOg8dHR3uwQcfdHV1da6pqcm999577rvf/a677LLL3LFjx6yXnjDLly93oVDI1dTUuJaWluh29OjR6D733HOPGzdunNu2bZvbuXOnKy4udsXFxYarTryznYfGxkb34x//2O3cudM1NTW5LVu2uAkTJrhZs2YZrzzWoAiQc84999xzbty4cS41NdVNnz7d1dfXWy/pnFu8eLHLzc11qamp7tvf/rZbvHixa2xstF5W0r3//vtO0mnbkiVLnHMnP4r92GOPuezsbBcMBt2cOXNcQ0OD7aKT4Ezn4ejRo27u3LluzJgxLiUlxY0fP94tW7ZsyP2ftL7++SW59evXR/f54osv3L333uu+9a1vuVGjRrmFCxe6lpYWu0UnwdnOw4EDB9ysWbNcRkaGCwaD7tJLL3U/+tGPXDgctl34Kfh1DAAAEwP+PSAAwNBEgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJj4f4W4/AnknuSPAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_traindata = x_train[0]     #visualising image\n",
    "plt.imshow(x_traindata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b224f05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = x_train.reshape(60000, -1) \n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "512cb484",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardise(X):\n",
    "    mean = np.sum(X, axis = 0)/X.shape[0] #mean by sum along pixels(features) and divide by no. of pixels(features)\n",
    "    Z = (X-mean)#standardise x by x - mean \n",
    "    print(mean.shape)\n",
    "    return Z, mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0625769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z, mean = standardise(x)        #shape(10000, 784)\n",
    "z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54e9a8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Rz = np.dot(z.T, z)/z.shape[0]     #correlation matrix\n",
    "e, ev = linalg.eigh(Rz)    #eigen value and eigen vector calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5e5f209",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 784)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ev.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "50e05764",
   "metadata": {},
   "outputs": [],
   "source": [
    "e_index = np.argsort(e)[::-1]      #sorting eigen values and eigen vectors in descending order\n",
    "e_sorted = e[e_index]\n",
    "ev_sorted = ev[:, e_index]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9eaf7198",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = np.dot(z, ev_sorted)       #projection matrix\n",
    "p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "60efe12d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 784)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ev_sorted.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94a17eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0dccc4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_x = []               # will store reconstructed x    \n",
    "\n",
    "ev_tilted = ev_sorted[:, :284]     #truncating eigen vector from 24, 44, .... in loop\n",
    "p_cap = np.dot(z, ev_tilted)     #projected matrix for truncated ev\n",
    "xnew_cap = np.dot(p_cap, ev_tilted.T)   #reconstructed images\n",
    "xx_cap = xnew_cap + mean         #adding mean corrosponding to it\n",
    "result_x.append(xx_cap)          #appending images to array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "94458407",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_xx = np.asarray(result_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5bafb470",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 60000, 784)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_xx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "79231fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "im_final= []\n",
    "# fig = plt.figure(figsize= (50, 150))\n",
    "\n",
    "# ax = fig.add_subplot(15, 3, 1)\n",
    "for i in range(60000):\n",
    "    \n",
    "    im_final.append(result_xx[0][i].reshape(1, 28, 28))\n",
    "#     l2_norm.append(np.sqrt(np.sum(np.power((img_initial-im_final),2))))\n",
    "# diff = np.sum(np.power(img_initial-im_final, 2))\n",
    "# l2_norm.append(np.sqrt(diff))\n",
    "# ax.imshow(im_final, cmap='gray')\n",
    "# ax.set_title(\"p:\"+str(284), fontsize='40') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4f89f73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainpca = np.asarray(im_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ae43e208",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 1, 28, 28)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainpca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "33d3444d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names = y_train\n",
    "class_names[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b28ccf06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "12ea07b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hp\\AppData\\Local\\Temp\\ipykernel_11060\\3308096686.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test = torch.tensor(y_test)\n"
     ]
    }
   ],
   "source": [
    "X_train_pca = torch.tensor(trainpca)\n",
    "y_train_pca = torch.tensor(y_train)\n",
    "X_test = torch.tensor(x_test)\n",
    "y_test = torch.tensor(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1d27a3e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54000, 6000)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size = int(0.9 * len(X_train_pca))\n",
    "val_size = len(X_train_pca) - train_size\n",
    "train_size, val_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1ea6e4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_dataset = TensorDataset(X_train_pca, y_train_pca)\n",
    "test_dataset = TensorDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "156649d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset=train_val_dataset, lengths=[train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a1265b1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(211, 24, 40)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "train_dataloader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dataloader = DataLoader(dataset=val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dataloader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# Let's see no of batches that we have now with the current batch-size\n",
    "len(train_dataloader), len(val_dataloader), len(test_dataloader) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3bada38a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 1, 28, 28]) torch.Size([256])\n"
     ]
    }
   ],
   "source": [
    "for X, y in train_dataloader:\n",
    "    print(X.shape, y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "78bc1f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class LeNet5V1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.feature = nn.Sequential(\n",
    "            #1\n",
    "            nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, stride=1, padding=2),   # 28*28->32*32-->28*28\n",
    "            nn.Tanh(),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2),  # 14*14\n",
    "            \n",
    "            #2\n",
    "            nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1),  # 10*10\n",
    "            nn.Tanh(),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2),  # 5*5\n",
    "            \n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=16*5*5, out_features=120),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(in_features=120, out_features=84),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(in_features=84, out_features=10),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.classifier(self.feature(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a56ba513",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "========================================================================================================================\n",
       "Layer (type (var_name))                  Input Shape          Output Shape         Param #              Trainable\n",
       "========================================================================================================================\n",
       "LeNet5V1 (LeNet5V1)                      [1, 1, 28, 28]       [1, 10]              --                   True\n",
       "├─Sequential (feature)                   [1, 1, 28, 28]       [1, 16, 5, 5]        --                   True\n",
       "│    └─Conv2d (0)                        [1, 1, 28, 28]       [1, 6, 28, 28]       156                  True\n",
       "│    └─Tanh (1)                          [1, 6, 28, 28]       [1, 6, 28, 28]       --                   --\n",
       "│    └─AvgPool2d (2)                     [1, 6, 28, 28]       [1, 6, 14, 14]       --                   --\n",
       "│    └─Conv2d (3)                        [1, 6, 14, 14]       [1, 16, 10, 10]      2,416                True\n",
       "│    └─Tanh (4)                          [1, 16, 10, 10]      [1, 16, 10, 10]      --                   --\n",
       "│    └─AvgPool2d (5)                     [1, 16, 10, 10]      [1, 16, 5, 5]        --                   --\n",
       "├─Sequential (classifier)                [1, 16, 5, 5]        [1, 10]              --                   True\n",
       "│    └─Flatten (0)                       [1, 16, 5, 5]        [1, 400]             --                   --\n",
       "│    └─Linear (1)                        [1, 400]             [1, 120]             48,120               True\n",
       "│    └─Tanh (2)                          [1, 120]             [1, 120]             --                   --\n",
       "│    └─Linear (3)                        [1, 120]             [1, 84]              10,164               True\n",
       "│    └─Tanh (4)                          [1, 84]              [1, 84]              --                   --\n",
       "│    └─Linear (5)                        [1, 84]              [1, 10]              850                  True\n",
       "========================================================================================================================\n",
       "Total params: 61,706\n",
       "Trainable params: 61,706\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 0.42\n",
       "========================================================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.05\n",
       "Params size (MB): 0.25\n",
       "Estimated Total Size (MB): 0.30\n",
       "========================================================================================================================"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "model_lenet5v1 = LeNet5V1()\n",
    "summary(model=model_lenet5v1, input_size=(1, 1, 28, 28), col_width=20,\n",
    "                  col_names=['input_size', 'output_size', 'num_params', 'trainable'], row_settings=['var_names'], verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "815f60ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics import Accuracy\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params=model_lenet5v1.parameters(), lr=0.001)\n",
    "accuracy = Accuracy(task='multiclass', num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "633da908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0| Train loss:  0.59833| Train acc:  0.82818| Val loss:  0.23732| Val acc:  0.93378\n",
      "Epoch: 1| Train loss:  0.17747| Train acc:  0.94612| Val loss:  0.14693| Val acc:  0.95880\n",
      "Epoch: 2| Train loss:  0.11599| Train acc:  0.96543| Val loss:  0.10414| Val acc:  0.96901\n",
      "Epoch: 3| Train loss:  0.08573| Train acc:  0.97340| Val loss:  0.09108| Val acc:  0.97447\n",
      "Epoch: 4| Train loss:  0.06840| Train acc:  0.97880| Val loss:  0.07823| Val acc:  0.97831\n",
      "Epoch: 5| Train loss:  0.05445| Train acc:  0.98289| Val loss:  0.06407| Val acc:  0.98207\n",
      "Epoch: 6| Train loss:  0.04644| Train acc:  0.98574| Val loss:  0.06260| Val acc:  0.98319\n",
      "Epoch: 7| Train loss:  0.03911| Train acc:  0.98796| Val loss:  0.06923| Val acc:  0.98049\n",
      "Epoch: 8| Train loss:  0.03437| Train acc:  0.98945| Val loss:  0.06080| Val acc:  0.98249\n",
      "Epoch: 9| Train loss:  0.02978| Train acc:  0.99096| Val loss:  0.05465| Val acc:  0.98477\n",
      "Epoch: 10| Train loss:  0.02553| Train acc:  0.99248| Val loss:  0.05251| Val acc:  0.98596\n",
      "Epoch: 11| Train loss:  0.02128| Train acc:  0.99372| Val loss:  0.05547| Val acc:  0.98505\n",
      "Epoch: 12| Train loss:  0.01897| Train acc:  0.99472| Val loss:  0.05670| Val acc:  0.98424\n",
      "Epoch: 13| Train loss:  0.01662| Train acc:  0.99495| Val loss:  0.05197| Val acc:  0.98514\n",
      "Epoch: 14| Train loss:  0.01468| Train acc:  0.99589| Val loss:  0.05072| Val acc:  0.98486\n",
      "Epoch: 15| Train loss:  0.01256| Train acc:  0.99637| Val loss:  0.05630| Val acc:  0.98468\n",
      "Epoch: 16| Train loss:  0.01134| Train acc:  0.99685| Val loss:  0.05220| Val acc:  0.98498\n",
      "Epoch: 17| Train loss:  0.00979| Train acc:  0.99722| Val loss:  0.05958| Val acc:  0.98189\n",
      "Epoch: 18| Train loss:  0.01009| Train acc:  0.99717| Val loss:  0.05364| Val acc:  0.98547\n",
      "Epoch: 19| Train loss:  0.00756| Train acc:  0.99807| Val loss:  0.05287| Val acc:  0.98693\n",
      "Epoch: 20| Train loss:  0.00617| Train acc:  0.99844| Val loss:  0.05189| Val acc:  0.98698\n",
      "Epoch: 21| Train loss:  0.00566| Train acc:  0.99852| Val loss:  0.05549| Val acc:  0.98547\n",
      "Epoch: 22| Train loss:  0.00517| Train acc:  0.99883| Val loss:  0.05579| Val acc:  0.98624\n",
      "Epoch: 23| Train loss:  0.00618| Train acc:  0.99822| Val loss:  0.05898| Val acc:  0.98593\n",
      "Epoch: 24| Train loss:  0.00461| Train acc:  0.99891| Val loss:  0.05633| Val acc:  0.98633\n",
      "Epoch: 25| Train loss:  0.00691| Train acc:  0.99776| Val loss:  0.05792| Val acc:  0.98542\n",
      "Epoch: 26| Train loss:  0.00569| Train acc:  0.99830| Val loss:  0.06375| Val acc:  0.98396\n",
      "Epoch: 27| Train loss:  0.00517| Train acc:  0.99861| Val loss:  0.06673| Val acc:  0.98363\n",
      "Epoch: 28| Train loss:  0.00627| Train acc:  0.99817| Val loss:  0.06206| Val acc:  0.98779\n",
      "Epoch: 29| Train loss:  0.00297| Train acc:  0.99932| Val loss:  0.05644| Val acc:  0.98730\n",
      "Epoch: 30| Train loss:  0.00287| Train acc:  0.99924| Val loss:  0.06330| Val acc:  0.98563\n",
      "Epoch: 31| Train loss:  0.00192| Train acc:  0.99954| Val loss:  0.05553| Val acc:  0.98861\n",
      "Epoch: 32| Train loss:  0.00135| Train acc:  0.99978| Val loss:  0.05393| Val acc:  0.98861\n",
      "Epoch: 33| Train loss:  0.00069| Train acc:  0.99993| Val loss:  0.05393| Val acc:  0.98954\n",
      "Epoch: 34| Train loss:  0.00172| Train acc:  0.99952| Val loss:  0.06170| Val acc:  0.98710\n",
      "Epoch: 35| Train loss:  0.01606| Train acc:  0.99474| Val loss:  0.06882| Val acc:  0.98424\n",
      "Epoch: 36| Train loss:  0.00734| Train acc:  0.99759| Val loss:  0.06441| Val acc:  0.98644\n",
      "Epoch: 37| Train loss:  0.00313| Train acc:  0.99906| Val loss:  0.05983| Val acc:  0.98726\n",
      "Epoch: 38| Train loss:  0.00062| Train acc:  0.99998| Val loss:  0.05977| Val acc:  0.98749\n",
      "Epoch: 39| Train loss:  0.00039| Train acc:  1.00000| Val loss:  0.06000| Val acc:  0.98803\n",
      "Epoch: 40| Train loss:  0.00025| Train acc:  1.00000| Val loss:  0.05732| Val acc:  0.98942\n",
      "Epoch: 41| Train loss:  0.00020| Train acc:  1.00000| Val loss:  0.05769| Val acc:  0.98926\n",
      "Epoch: 42| Train loss:  0.00018| Train acc:  1.00000| Val loss:  0.06202| Val acc:  0.98847\n",
      "Epoch: 43| Train loss:  0.00016| Train acc:  1.00000| Val loss:  0.05904| Val acc:  0.98905\n",
      "Epoch: 44| Train loss:  0.00014| Train acc:  1.00000| Val loss:  0.06221| Val acc:  0.98847\n",
      "Epoch: 45| Train loss:  0.00012| Train acc:  1.00000| Val loss:  0.06130| Val acc:  0.98830\n",
      "Epoch: 46| Train loss:  0.00011| Train acc:  1.00000| Val loss:  0.05886| Val acc:  0.98910\n",
      "Epoch: 47| Train loss:  0.00010| Train acc:  1.00000| Val loss:  0.05952| Val acc:  0.98893\n",
      "Epoch: 48| Train loss:  0.00009| Train acc:  1.00000| Val loss:  0.06535| Val acc:  0.98810\n",
      "Epoch: 49| Train loss:  0.00008| Train acc:  1.00000| Val loss:  0.06082| Val acc:  0.98884\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import os\n",
    "EPOCHS = 50\n",
    "TLOSS = [] ## to store the the train loss after each epoch\n",
    "TACC = []  ## to store the the train accurancy after each epoch\n",
    "VLOSS = [] \n",
    "VACC = []\n",
    "for epoch in range(EPOCHS):\n",
    "    # Training loop\n",
    "    train_loss, train_acc = 0.0, 0.0\n",
    "    for X, y in train_dataloader:\n",
    "        \n",
    "        model_lenet5v1.train()\n",
    "        \n",
    "        y_pred = model_lenet5v1(X)\n",
    "        \n",
    "        loss = loss_fn(y_pred, y)\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        acc = accuracy(y_pred, y)\n",
    "        train_acc += acc\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    train_loss /= len(train_dataloader)\n",
    "    train_acc /= len(train_dataloader)\n",
    "    TLOSS.append(train_loss)\n",
    "    TACC.append(train_acc)\n",
    "    # Validation loop\n",
    "    val_loss, val_acc = 0.0, 0.0\n",
    "    model_lenet5v1.eval()\n",
    "    with torch.inference_mode():\n",
    "        for X, y in val_dataloader:\n",
    "            \n",
    "            y_pred = model_lenet5v1(X)\n",
    "            \n",
    "            loss = loss_fn(y_pred, y)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            acc = accuracy(y_pred, y)\n",
    "            val_acc += acc\n",
    "            \n",
    "        val_loss /= len(val_dataloader)\n",
    "        val_acc /= len(val_dataloader)\n",
    "        VLOSS.append(val_loss)\n",
    "        VACC.append(val_acc)\n",
    " \n",
    "    print(f\"Epoch: {epoch}| Train loss: {train_loss: .5f}| Train acc: {train_acc: .5f}| Val loss: {val_loss: .5f}| Val acc: {val_acc: .5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd9f169",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc9a445",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
